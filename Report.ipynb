{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "This repository is based on the collaboration and competition problem studied in the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893) program and demonstrates a variant of the PPO learning algorithm in a multi-agent environment with a continuous action space.\n",
    "\n",
    "![Actor](resources/agent.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. General description\n",
    "\n",
    "In this environment, two agents control rackets to bounce a ball over a net. If an agent hits the ball over the net, it receives a reward of +0.1. If an agent lets a ball hit the ground or hits the ball out of bounds, it receives a reward of -0.01. Thus, the goal of each agent is to keep the ball in play.\n",
    "\n",
    "The observation space consists of 8 variables corresponding to the position and velocity of the ball and racket. In each timestep the agent receives the state of the last three time steps and each agent receives its own, local observation. Two continuous actions are available, corresponding to movement toward (or away from) the net, and jumping.\n",
    "\n",
    "The task is episodic, and in order to solve the environment, the agents must get an average score of +0.5 (over 100 consecutive episodes, after taking the maximum over both agents). Specifically, after each episode, we add up the rewards that each agent received (without discounting), to get a score for each agent. This yields 2 (potentially different) scores. We then take the maximum of these 2 scores. This yields a single score for each episode.\n",
    "\n",
    "The environment is considered solved, when the average (over 100 episodes) of those scores is at least +0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PPO Algorithm with adaptions\n",
    "\n",
    "Given that each agent only has only local observations of the environment, i.e. it does not know the state of the other agent, we followed a naive approach to the problem and just applied a traditional reinforcement algorithm to the problem. We treat both agents completely separate, each only using it's local observation and considering the effects of the other agent as part a (non-stationary) environment. This approach turned out to be capable of handling non-stationarity of the problem sufficiently well and give good results.\n",
    "\n",
    "### 2.1 Used algorithm and adaptions\n",
    "\n",
    "The algorithm we used follows closely the algorithm described in section 2 of this [project on continuous control](https://github.com/numblr/drlnd-/blob/master/Report.ipynb#2.-Algorithm). This algorithm is a variant of *Proximal Policy Optimization (PPO)* with *Generalized Advantage Estimation (GAE)* and a clipped objective function. As a critic an estimate of the value function is used. In addition, the algorithm uses a finite horizon instead of reward discounting, which is implemented using sliding windows over an episode. To keep this part short we moved the detailed description of the algorithm to the [Appendix](#5.-Appendix:-Varinant-of-the-PPO-Algorithm) and only discuss the modifications made to the algorithm in this section.\n",
    "\n",
    "The algorithm described above was successfully applied to a continuous control problem ([Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) enviroment) were a double-jointed arm is chasing a moving target. The original task was quasi-continuous, capped to episodes of 1000 timesteps. The original implementation of the algorithm was capable of training on multiple parallel agents, each acting on it's own separate environment. In the following we discuss the adaptions that were made to the algorithm in order to apply it to the current problem:\n",
    "\n",
    "#### 2.1.1. Exploration vs Exploitation\n",
    "\n",
    "In the original algorithm the policy on the continuous action space was modeled by a DNN model to estimate the mean of a normal distribution with a given, fixed variance. This model is on the one hand capable of handling the continuous action space and on the other hand takes care of adding exploration to the policy due to the non-zero variance. The problem, to which the algorithm was initally applied, worked well with this simple model and tolerated a relatively high variance without the policy becomming unpredictable.\n",
    "\n",
    "The problem examined in this project, however, is much more sensitive: High variance leads to the inability to exploit the current policy, i.e. the actions sampled from the policy are too inaccurate to give a good result. Contrary, too low variance leads to a lack of exploration, and the agent quickly get's stuck in a situation were he never encounters a useful action from which it could learn.\n",
    "\n",
    "This was solved by using a mixture of a normal distribution with high variance for exploration and a normal distribution with low variance for exploitation:\n",
    "\n",
    "$$\n",
    "    \\pi(a|s) = (1 - \\epsilon) \\mathcal{N}(\\hat{\\mu}_\\theta(s), \\sigma_{\\rm exploit})\n",
    "        + \\epsilon \\mathcal{N}(\\hat{\\mu}_\\theta(s), \\sigma_{\\rm explore})\n",
    "$$\n",
    "\n",
    "\n",
    "#### 2.1.2 Converting the episodic task to a quasi-continuous task\n",
    "\n",
    "The original task was quasi-continuous, capped to an episode of 1000 timesteps. The used algorithm implements a finite horizon for the agent by using a sliding window on the episode. Besides providing a finite horizon, this technique also allows to very effectively reduce noise during training by filtering out windows were no positive reward was achieved. To apply this strategy in the current setting again, during training episodes were concatenated to sequences of 1000 timesteps, and terminal states were penalized with a reward of -1 (during training only) to reflect that we want to maximize reward *within a single* episode. \n",
    "\n",
    "#### 2.1.3 Model initialization\n",
    "\n",
    "Initialization of the model parameters for actor and critic was modified to initialize all parameters and bias close to zero. This provides a good starting point for learning in this environment.\n",
    "\n",
    "With this tweaks the original algorithm could be applied with minimal modification.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Further remarks on the algorithm and chosen hyper parameters\n",
    "\n",
    "* During training multiple episodes are concatenated to a sequence of 1152 steps.\n",
    "* During training multiple the reward of terminal states in this sequence is modified to -1.\n",
    "* The horizon of the agent is limited to 72 steps during the first 128 epochs, to 96 steps until epoch 256 and 128 steps after\n",
    "* Sliding windows corresponding to the horizon with a offset of 2 steps are extracted, from which windows where no positive reward is obtained in any timestep are filtered out.\n",
    "* No reward discounting is used as we already limit the horizon of the agent. \n",
    "* The surrogate objective clipped with a window size of 0.1.\n",
    "* The parameter for the GAE is set to 0.1 to provide large variance reduction.\n",
    "* The DNN models used are simple and have both an input size of 24, hidden layers of size 128 and 64, and and output layer of 2 with tanh activation for the actor net and an output layer of size 1 for the critic. (see also [section](#5.3-Approximators-and-training) of the appendix).\n",
    "* Parameters and bias in the used DNN models are initilized around zero. \n",
    "* After generating 8 sequences of 1152 steps, learning is executed for 2 epochs on the sampled data in batches of size 32 and a learning rate of $10^{-3}$.   \n",
    "* Gradient clipping is applied to maximize the norm of the gradient to 1.\n",
    "* The variance for the normal distributions is set to 0.5 for exploration and 0.1 for exploitation, mix with an $\\epsilon$-value of 0.25."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.6 Results and performance\n",
    "The following plot shows the scores for each generated episode during the training process. The plot also includes the average over the last 100 episodes. The average crosses the +0.5 mark around the 25000 episodes and reaches a top of about +0.8. Note, however, that during training we add a significant amount of noise to the policy, thus the scores during training are way below the score the trained agent achieves with the policy without adding noise. For this reason it is also hard to tell at what point the trained agent would solve the environment. \n",
    "\n",
    "![Scores](scores.png)\n",
    "\n",
    "**As shown in the plot, the agent reaches an average score of +0.3 on 100 episodes after about than 25000 episodes during training. The trained agent achieves an average score of around 2.6 on 100 episodes when [executing the policy without noise](#4.6-Evaluation).**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.7 Possible Improvements\n",
    "\n",
    "The trained agent delivers quite good results and mostly reaches the hard coded limit of 1000 timesteps for the game. Nevertheless, there are quite a couple of points for improvement:\n",
    "\n",
    "### 2.7.1 Sample efficiency\n",
    "\n",
    "The first evident point that can be improved is sample efficiency. The main issue with the applied algorithm is that it generates full episodes before learning with a rather low level of reuse. It should be expected that temporal difference methods like DDPG combined with a replay buffer work much more sample efficient, as the can on the one hand improve the policy already during episode execution and on the other hand they can provide reuse through the replay buffer more efficiently. In combination with priority sampling this should also be a good replacement for the windowing strategy applied in this algorithm.\n",
    "\n",
    "### 2.7.2 Noise\n",
    "\n",
    "The noise used by the applied algorithm seems to be one of the most crucial points in this problem, as the balance between exploration and exploitation is quites sensitive in this problem. An improvement over the naive model applied in the presented solution could be the application of an [Ornstein-Uhlenbeck](https://en.wikipedia.org/wiki/Ornstein%E2%80%93Uhlenbeck_process) noise process, which is a modified random walk that offers a constant long-term distribution. Such a noise process has the advantage that it provides low variance in the short term, which supports exploitation, as actions remain more predictable, while providing high varince in the long term, which supports exploration.   \n",
    "\n",
    "### 2.7.3 Non-stationarity\n",
    "\n",
    "Though non-stationarity of the environment turned out to be not a crucial issue in this problem, is in general a problem in multi-agent environments, so it should be expected that the presented solution does not work as nicely in other environments. A straight forward improvement in this direction would be to extend the critic to depend on the state of both agents in the game to make the training process more stable. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Implementation\n",
    "\n",
    "In this section we provide a detailed walk through the implementation. To reproduce the above results, however, run the *coball.py* script from the command line as described in the README without command line parameters.\n",
    "\n",
    "First we run some neccessary imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pkg_resources\n",
    "import random\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.distributions.normal as dist\n",
    "from torch import optim\n",
    "from torch import autograd\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from unityagents import UnityEnvironment\n",
    "from unityagents.exception import UnityEnvironmentException\n",
    "\n",
    "from coball.util import plot\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "PLATFORM_PATHS = ['Tennis.app',\n",
    "        'Tennis_Windows_x86/Tennis.exe',\n",
    "        'Tennis_Windows_x86_64/Tennis.exe',\n",
    "        'Tennis_Linux/Tennis.x86',\n",
    "        'Tennis_Linux_NoVis/Tennis.x86',\n",
    "        'Tennis_Linux/Tennis.x86_64',\n",
    "        'Tennis_Linux_NoVis/Tennis.x86_64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Start the Environment\n",
    "\n",
    "The environment the agent operates on is based on a pre-built [Unity ML-Agent](https://github.com/Unity-Technologies/ml-agents) and is provided by the *CoBallEnv* class in the *.environment* package.\n",
    "\n",
    "This class encapsulates the pre-built Unity environment, which must be downloaded to one of the following locations **_before running the code cells below_**:\n",
    "\n",
    "- **Mac**: `\"coball/resources/Tennis.app\"`\n",
    "- **Windows** (x86): `\"coball/resources/Tennis_Windows_x86/Tennis.exe\"`\n",
    "- **Windows** (x86_64): `\"coball/resources/Tennis_Windows_x86_64/Tennis.exe\"`\n",
    "- **Linux** (x86): `\"coball/resources/Tennis_Linux/Tennis.x86\"`\n",
    "- **Linux** (x86_64): `\"coball/resources/Reacher_Linux/Tennis.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"coball/resources/Tennis_Linux_NoVis/Tennis.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"coball/resources/Tennis_Linux_NoVis/Tennis.x86_64\"`\n",
    "\n",
    "It is important that the */resources* folder **_only contains the agent for your operating system_**!\n",
    "\n",
    "If the code cell below returns an error, please revisit the installation instructions in the README and double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s CoBallEnv coball/environment\n",
    "class CoBallEnv:\n",
    "    \"\"\"Continuous control environment.\n",
    "\n",
    "    The environment accepts actions and provides states and rewards in response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        for path in PLATFORM_PATHS:\n",
    "            try:\n",
    "                unity_resource = pkg_resources.resource_filename('coball', 'resources/' + path)\n",
    "                self._env = UnityEnvironment(file_name=unity_resource)\n",
    "                print(\"Environment loaded from \" + path)\n",
    "                break\n",
    "            except UnityEnvironmentException as e:\n",
    "                print(\"Attempted to load \" + path + \":\")\n",
    "                print(e)\n",
    "                print(\"\")\n",
    "                pass\n",
    "\n",
    "        if not hasattr(self, '_env'):\n",
    "            raise Exception(\"No unity environment found, setup the environment as described in the README.\")\n",
    "\n",
    "        # get the default brain\n",
    "        self._brain_name = self._env.brain_names[0]\n",
    "        self._brain = self._env.brains[self._brain_name]\n",
    "\n",
    "        self._info = None\n",
    "        self._scores = None\n",
    "        self._score_history = ()\n",
    "\n",
    "    def generate_episode(self, agent, max_steps=None, episodic=True, train_mode=False):\n",
    "        \"\"\"Create a generator for and episode driven by an actor.\n",
    "        Args:\n",
    "            actor: An actor that provides the next action for a given state.\n",
    "            max_steps: Maximum number of steps (int) to take in the episode. If\n",
    "                None, the episode is generated until a terminal state is reached.\n",
    "\n",
    "        Returns:\n",
    "            A generator providing a tuple of the current state, the action taken,\n",
    "            the obtained reward, the next state and a flag whether the next\n",
    "            state is terminal or not.\n",
    "        \"\"\"\n",
    "        states = self.reset(train_mode=train_mode)\n",
    "        is_terminal = False\n",
    "        count = 0\n",
    "\n",
    "        while (max_steps is None or count < max_steps) and (not episodic or not is_terminal):\n",
    "            actions = agent.act(states)\n",
    "            rewards, next_states, is_terminals = self.step(actions, episodic)\n",
    "\n",
    "            step_data = (states, actions, rewards, next_states, is_terminals)\n",
    "\n",
    "            states = next_states\n",
    "            is_terminal = np.any(is_terminals)\n",
    "            count += 1\n",
    "\n",
    "            if np.any(np.isnan(states)):\n",
    "                raise ValueError(\"Nan from env\")\n",
    "\n",
    "            if is_terminal and not episodic and not count == max_steps:\n",
    "                self._score_history += (np.mean(self._scores), )\n",
    "                self._scores = np.zeros(self.get_agent_size())\n",
    "\n",
    "            yield step_data\n",
    "\n",
    "        self._score_history += (self.get_score(), )\n",
    "        self._scores = None\n",
    "\n",
    "    def reset(self, train_mode=False):\n",
    "        \"\"\"Reset and initiate a new episode in the environment.\n",
    "\n",
    "        Args:\n",
    "            train_mode: Indicate if the environment should be initiated in\n",
    "                training mode or not.\n",
    "\n",
    "        Returns:\n",
    "            The initial state of the episode (np.array).\n",
    "        \"\"\"\n",
    "        # if self._info is not None and not np.any(self._info.local_done):\n",
    "        #     raise Exception(\"Env is active, call terminate first\")\n",
    "\n",
    "        if self._scores is not None:\n",
    "            self._score_history += (np.mean(self._scores), )\n",
    "\n",
    "        self._info = self._env.reset(train_mode=train_mode)[self._brain_name]\n",
    "        self._scores = np.zeros(self.get_agent_size())\n",
    "\n",
    "        return self._info.vector_observations\n",
    "\n",
    "    def step(self, actions, episodic=True):\n",
    "        \"\"\"Execute an action on all instances.\n",
    "\n",
    "        Args:\n",
    "            action: An tensor of ints representing the actions each instance.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the rewards (floats), the next states (np.array) and\n",
    "            a booleans indicating if the next state is terminal or not.\n",
    "        \"\"\"\n",
    "        if self._info is None:\n",
    "            raise Exception(\"Env is not active, call reset first\")\n",
    "\n",
    "        if torch.is_tensor(actions):\n",
    "            actions = actions.numpy()\n",
    "\n",
    "        self._info = self._env.step(actions)[self._brain_name]\n",
    "        next_states = self._info.vector_observations\n",
    "        rewards = self._info.rewards\n",
    "        is_terminals = self._info.local_done\n",
    "        self._scores += np.array(rewards)\n",
    "\n",
    "        if not episodic:\n",
    "            rewards = [ -1.0 if x < 0 else x for x in rewards ]\n",
    "\n",
    "        return rewards, next_states, is_terminals\n",
    "\n",
    "    def terminate(self):\n",
    "        self._info = None\n",
    "        if self._scores is not None:\n",
    "            self._score_history += (self.get_score(), )\n",
    "        self._scores = None\n",
    "\n",
    "    def close(self):\n",
    "        self.terminate()\n",
    "        self._env.close()\n",
    "\n",
    "    def get_score(self):\n",
    "        \"\"\"Return the cumulative average reward of the current episode.\"\"\"\n",
    "        return np.max(self._scores) if self._scores is not None else self._score_history[-1]\n",
    "\n",
    "    def get_score_history(self):\n",
    "        \"\"\"Return the cumulative average reward of all episodes.\"\"\"\n",
    "        return self._score_history\n",
    "\n",
    "    def clear_score_history(self):\n",
    "        \"\"\"Clear the cumulative average reward of all episodes.\"\"\"\n",
    "        self._scores = None\n",
    "        self._score_history = ()\n",
    "\n",
    "    def get_agent_size(self):\n",
    "        if self._info is None:\n",
    "            raise ValueError(\"No agents are initialized\")\n",
    "\n",
    "        return len(self._info.agents)\n",
    "\n",
    "    def get_action_size(self):\n",
    "        return self._brain.vector_action_space_size\n",
    "\n",
    "    def get_state_size(self):\n",
    "        return 3 * self._brain.vector_observation_space_size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate episodes on the environment the *CoBallAgent* class provides an agent that selects actions in a particular state of the environement based on a policy function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s CoBallAgent coball/environment\n",
    "class CoBallAgent:\n",
    "    \"\"\"Agent based on a policy approximator.\"\"\"\n",
    "\n",
    "    def __init__(self, policy):\n",
    "        \"\"\"Initialize the agent.\n",
    "\n",
    "        Args:\n",
    "            pi: policy-function that is callable with n states and returns a\n",
    "                (n, a)-dim array-like containing the value of each action.\n",
    "        \"\"\"\n",
    "        self._policy = policy\n",
    "\n",
    "    def act(self, states):\n",
    "        \"\"\"Select actions for the given states.\n",
    "\n",
    "        Args:\n",
    "            state: An array-like of states to choose the actions for.\n",
    "        Returns:\n",
    "            An array-like of floats representing the actions.\n",
    "        \"\"\"\n",
    "        if not torch.is_tensor(states):\n",
    "            try:\n",
    "                states = torch.from_numpy(states, dtype=torch.float)\n",
    "            except:\n",
    "                states = torch.from_numpy(np.array(states, dtype=np.float))\n",
    "        else:\n",
    "            states = states.float()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            return self._policy.sample(states)\n",
    "\n",
    "    def get_policy(self):\n",
    "        return self._policy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can setup and test the environment and run an episode with a random or dummy policy. To run the test, uncomment the *test_env()* invocation and choose the policy you like.\n",
    "\n",
    "**_After you ran the test you need to comment it out again and restart the kernel, as recreating the environment does not work!_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment loaded from Tennis.app\n"
     ]
    }
   ],
   "source": [
    "env = CoBallEnv()\n",
    "\n",
    "def test_env():\n",
    "    # all actions are between -1 and 1\n",
    "    class DummyPolicy:\n",
    "        def sample(self, state):\n",
    "            return torch.rand(env.get_agent_size(), env.get_action_size()) * 2.0 - 1.0\n",
    "\n",
    "    agent = CoBallAgent(DummyPolicy())\n",
    "    episode = enumerate(env.generate_episode(agent, max_steps = 1000))\n",
    "    for count, step_data in episode:\n",
    "        # Consume the generated steps\n",
    "        pass\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    \n",
    "##### UNCOMMENT THIS TO TEST, COMMENT OUT AND RESTART THE KERNEL AFTERWARDS ####\n",
    "\n",
    "# test_env()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Examine the State and Action Spaces\n",
    "\n",
    "In this environment, a double-jointed arm can move to target locations. A reward of `+0.1` is provided for each step that the agent's hand is in the goal location. Thus, the goal of your agent is to maintain its position at the target location for as many time steps as possible.\n",
    "\n",
    "The observation space consists of `33` variables corresponding to position, rotation, velocity, and angular velocities of the arm.  Each action is a vector with four numbers, corresponding to torque applicable to two joints.  Every entry in the action vector must be a number between `-1` and `1`.\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "Size of each states: 24\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "states = env.reset()\n",
    "\n",
    "# number of agents\n",
    "num_agents = env.get_agent_size()\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = env.get_action_size()\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state_size = env.get_state_size()\n",
    "print('Size of each states:', state_size)\n",
    "\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], states.shape[1]))\n",
    "print('The state for the first agent looks like:', states[0])\n",
    "\n",
    "env.clear_score_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Models and policy\n",
    "\n",
    "The models to appoximate the action means of the policy distributions (*Actor*) and the state-value function (*Critic*) are implemented as PyTorch *nn.Module*. As they share the same hidden layers, they extend a common base class, and only add their individual output layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s PPOModel coball/model\n",
    "class PPOModel(nn.Module):\n",
    "    \"\"\"Base module for Actor and Critic models.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, output_size, seed, fc1_units, fc2_units):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            output (int): Dimension of the output\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(PPOModel, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, output_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s Actor coball/model\n",
    "class Actor(PPOModel):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed=2, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            action_size (int): Dimension of each action\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Actor, self).__init__(state_size, action_size, seed, fc1_units, fc2_units)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        self.fc1.bias.data.normal_(0.0, 3e-3)\n",
    "        self.fc1.weight.data.uniform_(*self._init_limits(self.fc1))\n",
    "        self.fc2.bias.data.normal_(0.0, 3e-3)\n",
    "        self.fc2.weight.data.uniform_(*self._init_limits(self.fc2))\n",
    "        self.fc3.bias.data.normal_(0.0, 3e-3)\n",
    "        self.fc3.weight.data.uniform_(0.0, 3e-3)\n",
    "\n",
    "    def _init_limits(self, layer):\n",
    "        input_size = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(input_size)\n",
    "\n",
    "        return (-lim, lim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an actor (policy) network that maps states -> action means.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return F.tanh(self.fc3(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s Critic coball/model\n",
    "class Critic(PPOModel):\n",
    "    \"\"\"Critic (Value) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, seed=2, fc1_units=128, fc2_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int): Dimension of each state\n",
    "            seed (int): Random seed\n",
    "            fc1_units (int): Number of nodes in first hidden layer\n",
    "            fc2_units (int): Number of nodes in second hidden layer\n",
    "        \"\"\"\n",
    "        super(Critic, self).__init__(state_size, 1, seed, fc1_units, fc2_units)\n",
    "        self._reset_parameters()\n",
    "\n",
    "    def _reset_parameters(self):\n",
    "        self.fc1.weight.data.uniform_(*self._init_limits(self.fc1))\n",
    "        self.fc2.weight.data.uniform_(*self._init_limits(self.fc2))\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def _init_limits(self, layer):\n",
    "        input_size = layer.weight.data.size()[0]\n",
    "        lim = 1. / np.sqrt(input_size)\n",
    "\n",
    "        return (-lim, lim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"Build an critic (value function) network that maps states -> values.\"\"\"\n",
    "        x = F.relu(self.fc1(state))\n",
    "        x = F.relu(self.fc2(x))\n",
    "\n",
    "        return self.fc3(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The policy itself is implemented in a separate class that wraps the mean approximator and provides the distribution function for each action. The *Policy* class can be used to calculate the log probabilities of given state-action pairs as well a to sample actions in a given state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s Policy coball/model\n",
    "class Policy():\n",
    "    \"\"\"Policy that provides action probabilities and samples actions accordingly.\n",
    "\n",
    "    The policy consists of a combination of two normal distributions, one with\n",
    "    high variance to drive exploration and one with low variance to drive\n",
    "    exploitation. The mean of both distributions is equal and provided by the\n",
    "    underlying model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, sigma=0.01, sigma_explore=0.5, epsilon=0.1, cap=[-1.0, 1.0], replay=False):\n",
    "        \"\"\"Initialize parameters.\n",
    "        Params\n",
    "        ======\n",
    "            model fn: state -> means: A model that maps states to means of the\n",
    "                  action distributions\n",
    "            sigma (float): variance of the action distribution for exploitation\n",
    "            sigma_explore (float): variance of the action distribution exploration\n",
    "            epsilon: weight of the exploration distribution\n",
    "            cap (list): limits for the action values\n",
    "        \"\"\"\n",
    "        if sigma <= 0.0:\n",
    "            raise ValueError(\"sigma must be positive: \" + str(sigma))\n",
    "\n",
    "        self._model = model\n",
    "        self._sigma = sigma\n",
    "        self._sigma_explore = sigma_explore\n",
    "        self._epsilon = epsilon\n",
    "        self._cap_min = cap[0] if cap is not None else None\n",
    "        self._cap_max = cap[1] if cap is not None else None\n",
    "        self._replay = replay\n",
    "\n",
    "    def log_prob(self, states, actions):\n",
    "        \"\"\"Log-probabilities of the given actions for the given states.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        states (tensor): dimensions (agents, steps, state_size)\n",
    "        actions (tensor): dimensions (agents, steps, action_size)\n",
    "\n",
    "        Returns\n",
    "        ======\n",
    "        log probabilities (tensor): dimensions (agents, steps, 1)\n",
    "        \"\"\"\n",
    "        if not states.dim() == actions.dim() == 3:\n",
    "            raise ValueError(\"dimensions are too small, unsqueeze: \" + str(states.size()) + \"-\" + str(actions.size()))\n",
    "\n",
    "        if not states.size()[:2] == actions.size()[:2]:\n",
    "            raise ValueError(\"dimenions don't match: \" + str(states.size()) + \"-\" + str(actions.size()))\n",
    "\n",
    "        states = states.to(device, dtype=torch.float)\n",
    "        actions = actions.to(device, dtype=torch.float)\n",
    "\n",
    "        dist_exploit, dist_explore = self.distributions(states)\n",
    "\n",
    "        prob_exploit = torch.exp(dist_exploit.log_prob(actions).float() + math.log(1 - self._epsilon))\n",
    "        prob_explore = torch.exp(dist_explore.log_prob(actions).float() + math.log(self._epsilon))\n",
    "        log_probs = torch.log(prob_exploit + prob_explore)\n",
    "\n",
    "        if self._cap_max is not None:\n",
    "            boundary = torch.ge(actions, self._cap_max)\n",
    "            if boundary.any():\n",
    "                cdf = (1 - self._epsilon) * (1 - dist_exploit.cdf(actions).float()) \\\n",
    "                    + self._epsilon * (1 - dist_explore.cdf(actions).float())\n",
    "                log_cdf = torch.log(cdf).float()\n",
    "                log_probs = torch.where(boundary, log_cdf, log_probs)\n",
    "        if self._cap_min is not None:\n",
    "            boundary = torch.le(actions, self._cap_min)\n",
    "            if boundary.any():\n",
    "                cdf = (1 - self._epsilon) * dist_exploit.cdf(actions).float() \\\n",
    "                        + self._epsilon * 1 - dist_explore.cdf(actions).float()\n",
    "                log_cdf = torch.log(cdf).float()\n",
    "                log_probs = torch.where(boundary, log_cdf, log_probs)\n",
    "\n",
    "        return torch.sum(log_probs, dim=2).unsqueeze(2)\n",
    "\n",
    "    def distributions(self, states):\n",
    "        means = self._model(states.to(device, dtype=torch.float))\n",
    "\n",
    "        return (dist.Normal(means, self._sigma), dist.Normal(means, self._sigma_explore))\n",
    "\n",
    "    def sample(self, states):\n",
    "        \"\"\"Sample actions for the given states according to the policy.\n",
    "\n",
    "        Params\n",
    "        ======\n",
    "        states (tensor): dimensions (agents, steps, state_size)\n",
    "\n",
    "        Returns\n",
    "        ======\n",
    "        actions (tensor): dimensions (agents, steps, action_size)\n",
    "        \"\"\"\n",
    "        if self._replay:\n",
    "            return self._model(states.to(device, dtype=torch.float))\n",
    "\n",
    "        dist_exploit, dist_explore = self.distributions(states)\n",
    "        if self._epsilon == 0.0 or random.uniform(0, 1) > self._epsilon:\n",
    "            sample = dist_exploit.sample()\n",
    "        else:\n",
    "            sample = dist_explore.sample()\n",
    "\n",
    "        if self._cap_min is None or self._cap_max is None:\n",
    "            return sample\n",
    "        else:\n",
    "            return torch.clamp(sample, self._cap_min, self._cap_max)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 4.4 PPO implementation\n",
    "\n",
    "The implementation of the algorithm is contained in the *PPOLearner* class that executes the the PPO algorithm with the given paramters. It also takes care of concatenating episodes and windowing: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load -s PPOLearner coball/training\n",
    "class PPOLearner():\n",
    "    \"\"\"Implementation of the PPO algorithm.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env=None,\n",
    "            window_size=72, window_step=2, rollout_length=1152,\n",
    "            episodes_in_epoch=8, ppo_epochs=2, batch_size=32,\n",
    "            sigma=0.1, sigma_explore=0.5, epsilon=0.25,\n",
    "            ppo_clip=0.1, gamma=1.0, gae_tau=0.1, lr=1e-3, grad_clip=1):\n",
    "        # Don't instantiate as default as the constructor already starts the unity environment\n",
    "        self._env = env if env is not None else CoBallEnv()\n",
    "\n",
    "        self._state_size = self._env.get_state_size()\n",
    "        self._actions = self._env.get_action_size()\n",
    "\n",
    "        self._rollout_length = rollout_length\n",
    "        self._episodes_in_epoch = episodes_in_epoch\n",
    "        self._ppo_epochs = ppo_epochs\n",
    "        self._batch_size = batch_size\n",
    "\n",
    "        self._window_size = window_size\n",
    "        self._window_step = window_step\n",
    "\n",
    "        self._sigma = sigma\n",
    "        self._sigma_explore = sigma_explore\n",
    "        self._epsilon = epsilon\n",
    "        self._gamma = gamma\n",
    "        self._gae_tau = gae_tau\n",
    "\n",
    "        self._ppo_clip = ppo_clip\n",
    "        self._grad_clip = grad_clip\n",
    "        self._lr = lr\n",
    "\n",
    "        self._policy_model = Actor(self._state_size, self._actions).to(device)\n",
    "        self._value_model = Critic(self._state_size).to(device)\n",
    "        self._set_learning_rate(lr)\n",
    "\n",
    "        self._policy_model.eval()\n",
    "        self._value_model.eval()\n",
    "\n",
    "        print(\"Initialize PPOLearner with model:\")\n",
    "        print(self._policy_model)\n",
    "        print(self._value_model)\n",
    "\n",
    "    def _set_learning_rate(self, lr):\n",
    "        self._policy_optimizer = optim.Adam(self._policy_model.parameters(), lr, eps=1e-5)\n",
    "        self._value_optimizer = optim.Adam(self._value_model.parameters(), lr, eps=1e-5)\n",
    "\n",
    "    def _adapt(self, epoch):\n",
    "        if epoch == 96:\n",
    "            self._set_learning_rate(self._lr/2)\n",
    "        if epoch == 128:\n",
    "            self._window_size = 96\n",
    "        if epoch == 256:\n",
    "            self._window_size = 128\n",
    "\n",
    "    def save(self, path, dir=\"results\"):\n",
    "        \"\"\"Store the learning result.\n",
    "\n",
    "        Store the parameters of the current models to the given path.\n",
    "        \"\"\"\n",
    "        torch.save(self._policy_model.state_dict(), os.path.join(dir, \"actor_\" + path))\n",
    "        torch.save(self._value_model.state_dict(), os.path.join(dir, \"critic_\" + path))\n",
    "\n",
    "    def load(self, path, dir=\"results\"):\n",
    "        \"\"\"Load learning results.\n",
    "\n",
    "        Load the parameters from the given path into the models.\n",
    "        \"\"\"\n",
    "        self._policy_model.load_state_dict(torch.load(os.path.join(dir, \"actor_\" + path)))\n",
    "        self._value_model.load_state_dict(torch.load(os.path.join(dir, \"critic_\" + path)))\n",
    "        self._policy_model.to(device)\n",
    "\n",
    "    def get_agent(self, epsilon=0.0, replay=False):\n",
    "        \"\"\"Return an agent based on the current policy model.\n",
    "        \"\"\"\n",
    "        return CoBallAgent(self.get_policy(epsilon=epsilon, replay=replay))\n",
    "\n",
    "    def get_policy(self, epsilon=0.0, replay=False):\n",
    "        \"\"\"Return a policy based on the model of the learner.\n",
    "        \"\"\"\n",
    "        return Policy(self._policy_model, sigma=self._sigma,\n",
    "                sigma_explore=self._sigma_explore, epsilon=self._epsilon, replay=replay)\n",
    "\n",
    "    def train(self, num_epochs=100):\n",
    "        for epoch in range(num_epochs):\n",
    "            self._adapt(epoch)\n",
    "\n",
    "            policy = self.get_policy(self._epsilon)\n",
    "\n",
    "            episodes = ( self._generate_episode(policy, epoch) for i in range(self._episodes_in_epoch) )\n",
    "            episodes = ( e for e in zip(*episodes) )\n",
    "\n",
    "            states, actions, rewards, next_states, is_terminals = \\\n",
    "                    [ torch.cat(component, dim=0).detach() for component in episodes ]\n",
    "\n",
    "            returns = self._calculate_returns(rewards).detach()\n",
    "            log_probs = policy.log_prob(states, actions).detach()\n",
    "            values = self._value_model(states).detach()\n",
    "            next_values = self._value_model(next_states).detach()\n",
    "\n",
    "            td_errors = rewards + self._gamma * next_values - values\n",
    "            advantages = self._calculate_advantages(td_errors)\n",
    "\n",
    "            advantages = (advantages - advantages.mean()) / advantages.std()\n",
    "\n",
    "            print(\"Collected windows: \" + str(states.size()))\n",
    "\n",
    "            # Validate dimensions\n",
    "            for data in (states, actions, next_states, rewards, is_terminals,\n",
    "                    returns, advantages, values, log_probs):\n",
    "                assert len(set([ d.size()[0] for d in data ])) == 1\n",
    "            for data in (states, actions, next_states, rewards, is_terminals,\n",
    "                    returns, advantages, values, log_probs):\n",
    "                assert len(set([ d.size()[1] for d in data ])) == 1\n",
    "            assert self._env.get_state_size() == states.size()[2] == next_states.size()[2]\n",
    "            assert self._env.get_action_size() == actions.size()[2]\n",
    "            for data in (rewards, is_terminals, returns, advantages, values, log_probs):\n",
    "                assert 1 == data.size()[2]\n",
    "\n",
    "            self._value_model.train()\n",
    "            self._policy_model.train()\n",
    "\n",
    "            for ppo_epoch in range(self._ppo_epochs):\n",
    "                sampler = [idx for idx in self._random_sample(np.arange(states.size(0)), self._batch_size) ]\n",
    "                for batch_indices in sampler:\n",
    "                    try:\n",
    "                        with autograd.detect_anomaly():\n",
    "                            batch_indices = torch.tensor(batch_indices).long()\n",
    "                            sampled_states, sampled_actions, sampled_log_probs_old, sampled_returns, sampled_advantages = [\n",
    "                                    data[batch_indices]\n",
    "                                    for data in [states, actions, log_probs, returns, advantages] ]\n",
    "\n",
    "                            new_log_probs = policy.log_prob(sampled_states.detach(), sampled_actions.detach())\n",
    "                            ratio = (new_log_probs - sampled_log_probs_old.detach()).exp()\n",
    "\n",
    "                            obj = ratio * sampled_advantages.detach()\n",
    "                            obj_clipped = ratio.clamp(1.0 - self._ppo_clip, 1.0 + self._ppo_clip) \\\n",
    "                                    * sampled_advantages.detach()\n",
    "                            policy_loss = - torch.min(obj, obj_clipped).mean()\n",
    "                            # if policy_loss != -obj.detach().mean():\n",
    "                            #     print(\"clipped\")\n",
    "\n",
    "                            new_value = self._value_model(sampled_states.detach())\n",
    "                            value_loss = 0.5 * (sampled_returns.detach() - new_value).pow(2).mean()\n",
    "\n",
    "                            self._value_optimizer.zero_grad()\n",
    "                            value_loss.backward()\n",
    "                            nn.utils.clip_grad_norm_(self._value_model.parameters(), self._grad_clip)\n",
    "                            self._value_optimizer.step()\n",
    "\n",
    "                            self._policy_optimizer.zero_grad()\n",
    "                            policy_loss.backward()\n",
    "                            nn.utils.clip_grad_norm_(self._value_model.parameters(), self._grad_clip)\n",
    "                            self._policy_optimizer.step()\n",
    "                    except BaseException as e:\n",
    "                        pass\n",
    "\n",
    "                self._value_model.eval()\n",
    "                self._policy_model.eval()\n",
    "\n",
    "                yield policy_loss, self._env.get_score(), ppo_epoch == self._ppo_epochs - 1\n",
    "\n",
    "    def _generate_episode(self, policy, epoch):\n",
    "        agent = CoBallAgent(policy)\n",
    "\n",
    "        episode = ( step_data for step_data in self._env.generate_episode(agent,\n",
    "                max_steps=self._rollout_length, episodic=False, train_mode=True) )\n",
    "        episode = ( episode_data for episode_data in zip(*episode) )\n",
    "\n",
    "        # state = tuple of (1,33) arrays, etc.., concat along first dimension\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                torch.stack(tuple(self._to_tensor(step)[0] for step in data), dim=1)\n",
    "                for data in episode ]\n",
    "\n",
    "        assert self._env.get_agent_size() == states.size()[0]\n",
    "\n",
    "        #split episodes\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                self._split(data, self._window_size)\n",
    "                for data in [states, actions, rewards, next_states, is_terminals] ]\n",
    "\n",
    "        positive_rewards = torch.any(rewards > 0.0, dim=1).squeeze()\n",
    "        states, actions, rewards, next_states, is_terminals = [\n",
    "                data[positive_rewards,:,:]\n",
    "                for data in [states, actions, rewards, next_states, is_terminals] ]\n",
    "\n",
    "        # Verify dimensions\n",
    "        assert states.size()[0] == actions.size()[0] == rewards.size()[0] \\\n",
    "                == next_states.size()[0] == is_terminals.size()[0]\n",
    "        assert self._env.get_state_size() == states.size()[2] == next_states.size()[2]\n",
    "        assert self._env.get_action_size() == actions.size()[2]\n",
    "        assert 1 == rewards.size()[2] == is_terminals.size()[2]\n",
    "\n",
    "        print(\"Generated episode: \" + str(states.size()[0]) + \"/\" + str(len(positive_rewards)) \\\n",
    "                + \" windows (\" + str(states.size()[1]) + \")\")\n",
    "\n",
    "        return (states, actions, rewards, next_states, is_terminals) if states.nelement() > 0 \\\n",
    "                else self._generate_episode(policy, epoch)\n",
    "\n",
    "    def _cat_component(self, episodes, component):\n",
    "        return torch.cat(episodes[component], dim=0)\n",
    "\n",
    "    def _split(self, x, split_size):\n",
    "        if x.size()[1] % split_size != 0:\n",
    "            raise ValueError(\"Illegal state, episode cannot be split: \" + str(x.size()))\n",
    "\n",
    "        splits = x.size()[1] // split_size\n",
    "        step = self._window_step\n",
    "\n",
    "        windows = x.view(splits*x.size()[0], split_size, x.size()[2])\n",
    "        shifted_windows = tuple([ x[:,i:-split_size+i,:].contiguous()\n",
    "                .view((splits-1)*x.size()[0], split_size, x.size()[2])\n",
    "                for i in range(step, split_size, step) ])\n",
    "\n",
    "        return torch.cat((windows,) + shifted_windows, dim=0)\n",
    "\n",
    "    def _to_tensor(self, *arrays, dtype=torch.float):\n",
    "        results = [ torch.tensor(a).to(device, dtype=dtype) if not torch.is_tensor(a) else a\n",
    "                for a in arrays ]\n",
    "\n",
    "        return tuple(result.unsqueeze(dim=1) if result.dim() == 1 else result\n",
    "                for result in results)\n",
    "\n",
    "    def _random_sample(self, indices, batch_size):\n",
    "        indices = np.asarray(np.random.permutation(indices))\n",
    "        batches = indices[:len(indices) // batch_size * batch_size].reshape(-1, batch_size)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "        r = len(indices) % batch_size\n",
    "        if r:\n",
    "            yield indices[-r:]\n",
    "\n",
    "    def _calculate_returns(self, rewards):\n",
    "        flipped = torch.flip(rewards, dims=(1,))\n",
    "        result = torch.zeros_like(flipped)\n",
    "        result[:,0,:] = flipped[:, 0, :]\n",
    "        for i in range(1, flipped.size()[1]):\n",
    "            result[:,i,:] = self._gamma * result[:,i-1,:] + flipped[:,i,:]\n",
    "\n",
    "        return torch.flip(result, dims=(1,))\n",
    "\n",
    "    def _calculate_advantages(self, td_errors):\n",
    "        flipped = torch.flip(td_errors, dims=(1,))\n",
    "        result = torch.zeros_like(flipped)\n",
    "        result[:,0,:] = flipped[:, 0, :]\n",
    "        for i in range(1, flipped.size()[1]):\n",
    "            result[:,i,:] = self._gamma * self._gae_tau * result[:,i-1,:] + flipped[:,i,:]\n",
    "\n",
    "        return torch.flip(result, dims=(1,))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 Training\n",
    "Now we can train the agent. To solve the environment we need to train the agent about 250 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from coball.util import print_progress\n",
    "\n",
    "episode_cnt = 0\n",
    "episode_step = 0\n",
    "for cnt, data in enumerate(PPOLearner(env=env).train(250)):\n",
    "    episode_step += 1\n",
    "    performance, score, terminal = data\n",
    "\n",
    "    if terminal:\n",
    "        episode_cnt += 1\n",
    "        episode_step = 0\n",
    "\n",
    "    print_progress(episode_cnt, episode_step, performance.item(), env.get_score_history(), total=2)\n",
    "    if terminal:\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We leave the execution of the training in this notebook to the reader, but be aware that it might take about 2 hours or more. A logfile of a training run with 400 epochs can be found [here](results/training.log).\n",
    "\n",
    "\n",
    "### 4.6 Evaluation\n",
    "\n",
    "The related score plot during the learning process looks like this:\n",
    "\n",
    "![Scores](scores.png)\n",
    "\n",
    "**As shown in the plot, the agent reaches an average score of +0.3 on 100 episodes after about than 25000 episodes during training. The trained agent achieves an average score of around 2.5 on 100 episodes when executing the policy without noise:**\n",
    "\n",
    "### 4.6 Running the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PPOLearner with model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=24, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tkb/anaconda3/envs/drlnd/lib/python3.6/site-packages/torch/nn/functional.py:1320: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "The agent just played an other episode with 1001 timesteps - be patient ;)\n",
      "Average score on 100 episodes: 2.6090000388771295\n"
     ]
    }
   ],
   "source": [
    "learner = PPOLearner(env=env)\n",
    "learner.load(\"parameters.pt\")\n",
    "replay_agent = learner.get_agent(replay=True)\n",
    "\n",
    "for i in range(100):\n",
    "    episode = env.generate_episode(replay_agent, train_mode=True)\n",
    "    length = 0\n",
    "    for _ in episode:\n",
    "        # Consume the generated steps\n",
    "        length += 1\n",
    "    if i % 10 == 9:\n",
    "        print(\"The agent just played an other episode with \" + str(length) + \" timesteps - be patient ;)\")\n",
    "\n",
    "print(\"Average score on 100 episodes: \" + str(np.mean(env.get_score_history())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The agents mostly hit\n",
    "the built in limit of 1001 time steps for an episode an achives an average of __2.6__ over 100 episodes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally let's watch the trained agents playing a game! We load the stored parameter files and execute the agent. Not that again for execution we set the policy to not add noise to the estimates of the actor model, but use the estimated means directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialize PPOLearner with model:\n",
      "Actor(\n",
      "  (fc1): Linear(in_features=24, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "Critic(\n",
      "  (fc1): Linear(in_features=24, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n",
      "Score:    2.600000038743019\n"
     ]
    }
   ],
   "source": [
    "learner = PPOLearner(env=env)\n",
    "learner.load(\"parameters.pt\")\n",
    "replay_agent = learner.get_agent(replay=True)\n",
    "\n",
    "episode = env.generate_episode(replay_agent)\n",
    "\n",
    "for count, step_data in enumerate(episode):\n",
    "    # Consume the generated steps\n",
    "    pass\n",
    "\n",
    "print(\"Score:    \" + str(env.get_score()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Appendix: Varinant of the PPO Algorithm\n",
    "\n",
    "Given that each agent only has a local observation of the environment, i.e. it does not know the state of the other agent we followed a naive approach to the problem and just applied a traditional reinforcement algorithm to the problem, which turned out to be capable of handling non-stationarity of the problem sufficiently well. The algorithm used is a variant of Proximal Policy Optimization (PPO) with a Generalized Advantage Estimation (GAE) and a clipped objective function. The algorithm uses a finite horizon instead of reward discounting. The algorithm follows closely the algorithm described in section 2 of this [project on continuous control](https://github.com/numblr/drlnd-/blob/master/Report.ipynb#2.-Algorithm). For convenience we include the description in section .... To make the algorithm work for the problem examined in this workspace the following modifications were made:\n",
    "\n",
    "### 5.1 Proximal Policy Optimization (PPO)\n",
    "\n",
    "Proximal Policy Optimization is a variant of policy gradient methods that is used to improve sample efficiency. Policy gradient methods are based on the objective to maximize the expectation $\\mathbb{E}_{\\tau \\sim \\pi_\\theta}\\left[R(\\tau)\\right]$ of the total reward $R(\\tau)$ for trajectories $\\tau=(s_0, a_0, r_0, s_1, a_1, r_1, \\dots)$ under a parameterized policy $\\pi_\\theta(\\tau)$.\n",
    "\n",
    "The gradient of the above objective can be derived as \n",
    "\n",
    "$$\n",
    "g\n",
    "= \\nabla_\\theta \\, \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\!\\! \\left[ \\sum_{t=0}^H \\log{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^H \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "$$\n",
    "\n",
    "where $R_f^t$ denotes the future reward at time step $t$ and $b(s_t)$ is a baseline which can depend on the state $s_t$. An equivalent formulation can be obtained by looking at the expectation with respect to a different policy $\\pi_{\\theta'}$:\n",
    "\n",
    "$$\n",
    "g\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^H \\frac{\\nabla_\\theta \\pi_\\theta(a_t|s_t)}{\\pi_\\theta(a_t|s_t)} (R_t^f - b(s_t) \\right]\n",
    "= \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}  \\left[ \\sum_{t=0}^H \\frac{\\left. \\nabla_\\theta \\pi_\\theta(a_t|s_t) \\right|_{\\theta = \\theta'}}{\\pi_{\\theta'}(a_t|s_t)} (R_t^f - b(s_t) \\right].\n",
    "$$\n",
    "\n",
    "When the above expectations are approximated by data from sampled trajectories, this implies that we can estimate the gradient for an evolved policy $\\pi_\\theta$ from trajectories that were sampled using the old policy $\\pi_{\\theta'}$ reasonably well if $\\theta$ is in a proximity of $\\theta'$. In other words, we can reuse trajectories generated with $\\pi_{\\theta'}$ to further optimize $\\pi_{\\theta}$, as long as $\\pi_{\\theta}$ does not get too far from $\\pi_{\\theta'}$.\n",
    "The above gradient can be obtained by diffentiating the objective function \n",
    "\n",
    "$$\n",
    "L(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}}  \\left[ \\sum_{t=0}^H \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta'}(a_t|s_t)} (R_t^f - b(s_t) \\right].\n",
    "$$\n",
    "\n",
    "As proposed in this [article](https://arxiv.org/abs/1707.06347), an easy and efficient way protect against the inacurracy introduced by a too large deviation of $\\theta$ from $\\theta'$ is to clip the ratio $r(\\theta) = \\frac{\\pi_{\\theta}}{\\pi_{\\theta'}}$ to a maximum value of $1 + \\epsilon$\n",
    "\n",
    "$$\n",
    "L^{PPO}(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_{\\theta'}} \\left [\n",
    "    \\min(r(\\theta)(R_t^f - b(s_t)), \\rm{clip}(r(\\theta), 1-\\epsilon, 1+\\epsilon) (R_t^f - b(s_t)))\n",
    "\\right],\n",
    "$$\n",
    "\n",
    "where the clipping parameter $\\epsilon$ is a hyper parameter of the algorithm.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Generalized Advantage Estimation (GAE) Critic\n",
    "\n",
    "As a baseline $b(s_t)$ we use the state-value function $V_{\\pi_\\theta}(s_t)$, and replace the term $(R_t^f - V_{\\pi_\\theta}(s_t))$ with a function approximator $\\tilde A(s, a)$ (critic). $\\tilde A(s, a)$ is an approximation of the advantage function $A_{\\pi_\\theta}(s, a) = Q_{\\pi_\\theta}(s,a) - V_{\\pi_\\theta}(s)$. To obtain this estimate we use the generalized advantage estimator as described in [this article](https://arxiv.org/abs/1506.02438), which approximates the advantage function using the state-value function as a weighted average over n-step temporal differences $r_t + r_{t+1} + \\cdots + r_{t+n-1} + V(s_{t+n}) - V(s_{t})$:\n",
    "\n",
    "$$\n",
    "\\tilde A^{GAE} = - \\tilde V_{\\pi_\\theta}(s_t) + (1-\\lambda)\\sum_{k=t}^H \\lambda^{(k-t)} \\left(\\sum_{l=t}^{k-1} r(s_l) + \\tilde V(s_k)\\right) = \\sum_{k=t}^H \\lambda^{(k-t)}\\left(r_{k} + \\tilde V(s_{k+1}) - \\tilde V(s_{k})\\right) = \\sum_{k=t}^H \\lambda^{(k-t)}\\delta_k^{\\tilde V},\n",
    "$$\n",
    "where $\\delta_t^V = r_t + V(s_{t+1}) - V(s_{t})$.\n",
    "\n",
    "The estimate $\\tilde V$ of the state-value is obtained by minimizing the error between the estimated and sampled state-values for a paramterized estimator $\\tilde V_\\phi(s_{t})$:\n",
    "$$\n",
    "L^V = \\hat{\\mathbb{E}} \\left[ \\| R_t^f - \\tilde V_\\phi(s_{t}) \\| \\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 Finite horizon and noise reduction\n",
    "\n",
    "Instead of discounting of rewards we decided to consider a fixed size finite horizon for the agent. This is implemented by generating full episodes, splitting them with a rolling window corresponding to the horizon of the agent and removing windows in which the agent does not obtain any reward. \n",
    "\n",
    "![windows](resources/windows.png)\n",
    "\n",
    "The reason for this is the following: Though the task is setup to be episodic, it is continuous in it's nature, and actions should have a comparable short term effect. From this observation it seems reasonable to assume that a finite horizon should be sufficient for the agent to solve the task. Further, looking at the actor at the beginning of the learning process, it is evident that it suffers from find a good starting point, as the probability to reach the target by accident is low. Implemening a fixed horizon instead of discounting gives us an easy way to remove windows in which the agent doesn't retrieve any reward. Considering that we can understand the learning process as a directed search driven by the obtained reward, it seems reasonable to assume that these windows do not contribute in a positive way to the learning process.\n",
    "\n",
    "### 5.3 Approximators and training\n",
    "\n",
    "For both the approximator $\\pi_\\theta$ as well as the state-value function $V_\\phi$ we use fully connected neural networds with two hidden layers of size 128 and 64 with relu activations. The two networks differ only in the output layers, as described in the following. The approximators are trained in batches and a replay buffer is used, i.e. the  order of timestpes is randomized during training.\n",
    "\n",
    "#### 5.3.1 Policy approximator\n",
    "\n",
    "The policy approximator maps a state to a normal distribution  with a fixed variance and an estimated mean for each of the available actions.  The normal distributions are truncated to the interval $[-1,1]$ to conform to the allowed values for the actions. The mean estimate is provided by a neural network as described above, where the size of the output layer corresponds to the number of available actions and uses a $\\tanh$ activation function to confine the output to the interval $[-1,1]$.\n",
    "\n",
    "![Actor net](resources/actor_net.png)\n",
    "\n",
    "#### 5.3.2 State-Value approximator\n",
    "\n",
    "The approximator for the state value function maps a state to a real number and is implemented by a neural network as described above with a single node in the output layer without additional activation.\n",
    "\n",
    "![Critic net](resources/critic_net.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 Algorithm:\n",
    "\n",
    "* Init $\\theta'$, $\\theta$, $\\phi$\n",
    "* For $N$ epochs:\n",
    "    * Set $\\theta' = \\theta$\n",
    "    * Generate $M$ episodes from each agent with respect to $\\pi_{\\theta'}$\n",
    "    * Split the episodes into windows of length $m$ using a rolling window\n",
    "    * Remove windows where no reward is achived\n",
    "    * Calculate $\\pi_{\\theta'}(a_t|s_t)$ and $A^{GAE}(s_t, a_t)$ for all time steps in each window\n",
    "    * For $k$ epochs:\n",
    "        * Update $\\theta$ to optimize $L^{PPO}$ using sampled a batches of $j$ windows\n",
    "        * Update $\\phi$ to optimize $L^V$ using the same batches as in the previous step\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
